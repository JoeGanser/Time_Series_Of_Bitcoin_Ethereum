{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Before the Bitcoin Bubble Burst: \n",
    "**Predicting day to day prices with ARIMA & Neural Nets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Joe Ganser** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ABSTRACT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The objective of this blog post is to experiment with time series techniques as well as methods in signal/noise extraction to make a prediction on the prices of both bitcoin and ethereum in the last week of August 2017, using all the previous pricing data.\n",
    "\n",
    "In this short term price forecast, I am predicting the next day's price based upon all the prices leading upto it. Then on the next day, I am repeating this process again, once next day's price is received. Model techniques like this could be used for high frequency trading, not long term investing. This is NOT about predicting long term trends.\n",
    "\n",
    "Because I am only predicting the next day out, and I am extracting the signal from the noise and what the models ACTUALLY predict is the noise. The predicted noise is then plugged back into the signal function, and that turns into the measurable prediction. This concept is explained a little more below.\n",
    "\n",
    "\n",
    "Lets take a look at the pricing data upto this point so far;\n",
    "\n",
    "<img src=\"Crypto_8_2017_Files/fullplots.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**WORKFLOW**\n",
    "\n",
    "The general process of how I built my models can be summarized in a few steps.\n",
    "\n",
    "1.Extract the signal from the noise (find stationarity via dickey fuller test). More about this below.\n",
    "\n",
    "2.Model the noise, and predict the noise for the next day (ARIMA and Neural nets)\n",
    "\n",
    "3.Plug the predicted noise into the signal, calculating the price value for the next day\n",
    "\n",
    "4.Compare the predicted price of the next day to the actual next day's price\n",
    "\n",
    "5.Repeat steps 2 and 3 once when obtaining the actual price of the next day  (use a for loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### MODEL CHOICES: ARIMA & NEURAL NET RECURSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The model choice was done simply for comparitive experimental reasons, only because ARIMA and Neural Net LSTM can predict time series. Other techniques could be done to, but I just wanted to compare these out of curiosity. Their performance metrics were the cumulative root mean square error across all the days predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### ASSUMPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The assumption, which is validated, is that the exponential long term trend shall remain over the last few days of August of 2017. The assumption was made through the techniques of signal noise extraction which are explained below. I do NOT assume that the exponential trend will go on forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# MODEL PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Here are the final results - the actual predictions of my models.\n",
    "<img src='Crypto_8_2017_Files/predictions.png'>\n",
    "\n",
    "Date|Bitcoin Price|ARIMA prediction|Neural Net Prediction\n",
    "----|-----|-----|\n",
    "2017-08-23|4318.35|4321.92|4369.75\n",
    "2017-08-24|4364.41|4368.25|4423.16\n",
    "2017-08-25|4352.30|4356.23|4397.79\n",
    "2017-08-26|4345.75|4349.69|4384.69\n",
    "2017-08-27|4390.31|4394.20|4430.15\n",
    "2017-08-28|4597.31|4601.04|4643.98\n",
    "2017-08-29|4583.02|4587.37|4643.36\n",
    "\n",
    "Date|Ethereum Price|ARIMA prediction|Neural Net Prediction\n",
    "----|-----|-----|\n",
    "2017-08-23|325.28|327.39|325.36\n",
    "2017-08-24|330.06|331.70|325.76\n",
    "2017-08-25|332.86|334.69|334.74\n",
    "2017-08-26|347.88|350.00|352.49\n",
    "2017-08-27|347.66|348.96|350.64\n",
    "2017-08-28|372.35|374.69|373.62\n",
    "2017-08-29|383.86|384.72|388.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**MODEL PERFORMANCE METRICS**\n",
    "\n",
    "So looking at these results, what were the model performance metrics? The performance metric I used to compare the results of the techniques was the summed root mean square error across all the days (seven, in total) predicted.\n",
    "\n",
    "\n",
    "\n",
    "| **Model**  | **Bitcoin RMSE** | **Ethereum RMSE**| \n",
    "| ---------- | ------- | ------ |\n",
    "| ARIMA      | 3.8985 | 9.73758| \n",
    "| Neural Net | 49.40448 | 1.67389   |\n",
    "\n",
    "Cumulative across the dates of Aug 23-Aug 29 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interesting Results!**\n",
    "\n",
    "ARIMA tended to perform better on bitcoin than neural nets, but the neural nets performed much better on the ethereum data! This may be related to the signal function I used, which is explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXTRACTING THE SIGNAL FROM THE NOISE USING THE AUGMENTED DICKEY FULLER TEST**\n",
    "\n",
    "\n",
    "<img src = 'Crypto_8_2017_Files/Signal_Noise.png'>\n",
    "\n",
    "The [augmented dickey fuller](https://en.wikipedia.org/wiki/Augmented_Dickeyâ€“Fuller_test) test is a statistical test to determine [stationarity](https://en.wikipedia.org/wiki/Stationary_process). Stationarity occurs when a series of data is transformed into format which clearly identifies the signal (aka a \"trend\") and all we are left with is noise. More specifically, stationarity occurs when we put the time series data is put into it's correct inverse function. With outputs similar to a Z-test, the test tells us if the new format we have has clearly identified the signal within the data. The null hypothesis of the Dickey Fuller test is that the signal and noise are NOT decoupled. The strategy behind using this is that by extracting the signal, we model and predict the noise and plug those predictions back into the signal.\n",
    "\n",
    "Stationarity data has four fundamental criteria:\n",
    "\n",
    "1. The average value is constant in time; $E[Y_t] = \\mu  \\space\\space  \\forall t\\in T$\n",
    "2. The variance is is finite for all time $Var[Y_t] = \\sigma^{2}<\\infty \\space\\space \\forall t\\in T$\n",
    "3. Covariance is dependent upon lagged values and not time itsself, i.e. $\\rho_k = \\frac{Cov(y_{t},y_{t+k})}{Var(y_t)} = \\frac{\\gamma_k}{\\gamma_0}$\n",
    "4. The joint distribution remains identical for all time, i.e. $[{Y(t_1),Y(t_2),...,Y(t_k)}] \\equiv [{Y(t_1+h),Y(t_2+h),...,Y(t_k+h)}] \\space\\space \\forall t,h\\in T $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**THE BEST FIT FUNCTION & IT'S INVERSE**\n",
    "\n",
    "Knowing the best fitting function is not enough to model the time series. What is needed is not only a best fit function, but that function's inverse as well. By putting the data of the time series into the inverse function, we can get the noise values. It's noise values that are put into the Dickey Fuller test.\n",
    "\n",
    "*A graphical example of an inverse function:* $log_{e}{x}$ is the inverse of $e^{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Crypto_8_2017_Files/InverseFunction.png'>\n",
    "\n",
    "\n",
    "Extracting the signal from the noise can be summed into three steps\n",
    "1. Find the a function similar to the time series data.\n",
    "2. Find that function's inverse.\n",
    "3. Plug the time series data into the inverse, and plug that transformed data set into \n",
    "    the Dickey Fuller Test\n",
    "    \n",
    "    *If we haven't rejected the null hypothesis in step 3 try step 1 over starting with a \n",
    "    different function.\n",
    "\n",
    "\n",
    "By looking at the very first plot, we can see that the trend of the cryptocurrency prices are in some way exponential. Thus, I can hypothesize that the signal is exponential in form.\n",
    "\n",
    "<img src='Crypto_8_2017_Files/bitprice_vs_exponential_curve.png'>\n",
    "\n",
    "The actual function form that allowed me to seperate the signal from the noise was indeed *exponential in form* but not simply exponential. It was by using this function's inverse that I could pass the dickey fuller test.\n",
    "\n",
    "The actual function that worked was this:\n",
    "\n",
    "|  **Concept Function & Definition**                   |\n",
    "| ------------------------- ------------------------------------------|\n",
    "|  Price on day t:           $X_t$|\n",
    "| Noise on day t: $\\delta_t$|\n",
    "|Price on day 0: $X_0$|\n",
    "| Signal Function:           $X_t = X_0 exp({\\sum_{i<t}{\\delta_i}})$ |  \n",
    "| Recursive form:           $X_t = X_{t-1} exp(\\delta_t)$           |\n",
    "| Inverse Function (noise):        $\\delta_t = \\ln\\frac{X_t}{X_{t-1}}$      |\n",
    "\n",
    "\n",
    "These function transformations worked with both the bitcoin & ethereum data.\n",
    "\n",
    "In code form, it is:\n",
    "\n",
    "`noise = (np.log(data['Price'])-np.log(data['Price'].shift())).dropna()`\n",
    "\n",
    "Where the `'.shift()'` cooresponds to the day before.\n",
    "\n",
    "**FAILING AND THEN PASSING THE DICKEY FULLER TEST**\n",
    "\n",
    "In the images passing the Dickey Fuller test without perfmoring any transformation, and then (botton of each tab), the Dickey Fuller test is passed (meaning we can reject the null hypothesis, and the data is stationary.)\n",
    "\n",
    "So now I have clearly identified the stationary form of the time series. These stationary forms can be plugged into the models to predict the noise, and using the noise predictions we can plug those back into the signal.\n",
    "\n",
    "The plots below show the series, the rolling mean and standard deviations plotted side by side. The first is what the original series looked like, and the second is what the series looks like after it's input into the inverse transformation function.\n",
    "\n",
    "<img src='Crypto_8_2017_Files/Dickey_Fuller_Tests.png'>\n",
    "\n",
    "The specific statistics for the tests were:\n",
    "\n",
    "Bitcoin Dickey-Fuller Test:|Results when transformed to stationary|Results results before stationary (failing)\n",
    "------|---------|--------|\n",
    "Test Statistic|-20.254025|3.433511\n",
    "p-value|0.000000|1.00000\n",
    "Number Lags Used|4.000000|28.0000\n",
    "Number of Observations Used| 2596.000000|2573.000000\n",
    "Critical Value (1%)|-3.962262|-3.962293\n",
    "Critical Value (5%)|-3.412183|-3.412198\n",
    "Critical Value (10%)|-3.128047|-3.128055\n",
    "\n",
    "Similar results were observed for ethereum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE FOR MODEL BUILDING & MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the prediction steps are structured in a FOR LOOP:**\n",
    "\n",
    "1. Plug the noise into the model\n",
    "2. Train the model on the history of the noise\n",
    "3. Predict the noise of the next day\n",
    "4. Plug the next day's predicted noise into the signal function.\n",
    "5. Add the actual price of the next day to the price data set, transform it into \n",
    "    stationarity and start again at step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELLING WITH ARIMA**\n",
    "\n",
    "There are several steps to building an ARIMA model. ARIMA operates on three parameters. Based upon the plots of auto-correlation, partial auto-correlation and the amount of Bayesian information criterion we select these three parameters. Ofcourse, the end goal is to minimize the root mean squared error of the model.\n",
    "\n",
    "**AUTO CORRELATION & PARTIAL AUTOCORRELATION PLOTS **\n",
    "\n",
    "The plots of auto-correlation & partial auto-correlation for bitcoin & ethereums stationary formats were:\n",
    "\n",
    "<img src='Crypto_8_2017_Files/correlation_plots.png'>\n",
    "\n",
    "Using these plots I can estimate the ARIMA parameters (p,q) of the parameters (p,q,d) for ARIMA models. Just by looking at the graphs it can be estimate that p & q are around no more than 7 each. After grid searching through lots of combinations it was found that **Bitcoins ARIMA parameters were (p=0,d=1,q=1)** and **Ethereum's ARIMA parameters were (p=2,d=0,q=0)**. These were validated by the minimization of the root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the ARIMA model code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA bitcoin predictions\n",
      "ARIMA Root Mean Squared Error:  3.898537928223503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted with ARIMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-23</th>\n",
       "      <td>4318.35</td>\n",
       "      <td>4321.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-24</th>\n",
       "      <td>4364.41</td>\n",
       "      <td>4368.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-25</th>\n",
       "      <td>4352.30</td>\n",
       "      <td>4356.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-26</th>\n",
       "      <td>4345.75</td>\n",
       "      <td>4349.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27</th>\n",
       "      <td>4390.31</td>\n",
       "      <td>4394.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>4597.31</td>\n",
       "      <td>4601.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29</th>\n",
       "      <td>4583.02</td>\n",
       "      <td>4587.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price  Predicted with ARIMA\n",
       "2017-08-23  4318.35               4321.92\n",
       "2017-08-24  4364.41               4368.25\n",
       "2017-08-25  4352.30               4356.23\n",
       "2017-08-26  4345.75               4349.69\n",
       "2017-08-27  4390.31               4394.20\n",
       "2017-08-28  4597.31               4601.04\n",
       "2017-08-29  4583.02               4587.37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ARIMA_predictions(original_series,stationary_series,parameters, days_out):\n",
    "    # Here we predict the noise\n",
    "    train = stationary_series[:-days_out]\n",
    "    # test the model on the last N points of the data, the models I used were at 7 days\n",
    "    test = stationary_series[-days_out:]\n",
    "    #\n",
    "    history = [x for x in train]\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "    predicted_values = []\n",
    "    tested = []\n",
    "    \n",
    "    #1. Plug the noise into the model\n",
    "    #2. Train the model on the history of the noise\n",
    "    #3. Predict the noise of the next day\n",
    "    #4. Plug the next day's predicted noise into the signal function.\n",
    "    #5. Add the actual price of the next day to the price data set, transform it into stationarity and start again at step 1.\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        model = ARIMA(history, order=parameters)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        yhat = float(model_fit.forecast(steps=1)[0])\n",
    "        predicted_values.append(yhat)\n",
    "        tested_values = list(test)[i]\n",
    "        tested.append(tested_values)\n",
    "        history.append(tested_values)\n",
    "    predictions_series = pd.Series(predicted_values, index = test.index)\n",
    "    \n",
    "    # This part couples the signal to the noise.\n",
    "    a = original_series.loc[original_series.index[-(days_out+1):]]['Price']\n",
    "    b = np.exp(predictions_series)\n",
    "    full_predictions = pd.DataFrame(a*b,columns=['Predicted with ARIMA']).dropna()\n",
    "    df = pd.concat([original_series.loc[original_series.index[-days_out:]],full_predictions],axis=1)\n",
    "    error = str(np.sqrt(mean_squared_error(df['Price'],df['Predicted with ARIMA'])))\n",
    "    print(\"ARIMA Root Mean Squared Error: \",error)\n",
    "    df.index.name = None\n",
    "    df[['Price','Predicted with ARIMA']] = df[['Price','Predicted with ARIMA']].apply(lambda x: round(x,2))\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"ARIMA bitcoin predictions\")\n",
    "bitcoin_ARIMA = ARIMA_predictions(bitcoin,bits_log_shift,(0,1,1),7)\n",
    "bitcoin_ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for Neural Net LSTM**\n",
    "\n",
    "The neural net system was based upon sequential, dense LSTM modelling techniques. Like the ARIMA model, it was dependent upon a stationary time series input, modeling the noise and then putting the noise predictions back into the signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin Neural Net Predictions\n",
      "Neural Net Root Mean Squared Error:  49.40448539455479\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Predicted with Neural Nets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-23</th>\n",
       "      <td>4318.35</td>\n",
       "      <td>4369.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-24</th>\n",
       "      <td>4364.41</td>\n",
       "      <td>4423.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-25</th>\n",
       "      <td>4352.30</td>\n",
       "      <td>4397.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-26</th>\n",
       "      <td>4345.75</td>\n",
       "      <td>4384.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-27</th>\n",
       "      <td>4390.31</td>\n",
       "      <td>4430.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>4597.31</td>\n",
       "      <td>4643.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-29</th>\n",
       "      <td>4583.02</td>\n",
       "      <td>4643.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price  Predicted with Neural Nets\n",
       "2017-08-23  4318.35                     4369.75\n",
       "2017-08-24  4364.41                     4423.16\n",
       "2017-08-25  4352.30                     4397.79\n",
       "2017-08-26  4345.75                     4384.69\n",
       "2017-08-27  4390.31                     4430.15\n",
       "2017-08-28  4597.31                     4643.98\n",
       "2017-08-29  4583.02                     4643.36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "def Neural_Net_predictions(original_time_series, stationary_time_series, days_out,nb_epoch,neurons):\n",
    "    # note all these \"sub\" functions are used on the stationary time series.\n",
    "    # The neural nets are used to predict the noise. Once the noise is predicted\n",
    "    # Its plugged back into the signal\n",
    "    \n",
    "    X = stationary_time_series\n",
    "    \n",
    "    # Step 2\n",
    "    # Break the time series into shifted components. Each column is a shifted value \n",
    "    # previously in the time series\n",
    "    def timeseries_to_supervised(data, lag=1):\n",
    "        df = pd.DataFrame(data)\n",
    "        columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "        columns.append(df)\n",
    "        df = pd.concat(columns, axis=1)\n",
    "        df.fillna(0, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Step 3\n",
    "    # We must put the time series onto the scale acceptable by the activation functions\n",
    "    def scale(train, test):\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        scaler = scaler.fit(train)\n",
    "        train = train.reshape(train.shape[0],train.shape[1])\n",
    "        train_scaled = scaler.transform(train)\n",
    "        test = test.reshape(test.shape[0],test.shape[1])\n",
    "        test_scaled = scaler.transform(test)\n",
    "        return scaler, train_scaled, test_scaled\n",
    "    \n",
    "    \n",
    "    # Step 4\n",
    "    def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "        X, y = train[:, 0:-1], train[:, -1]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        for i in range(nb_epoch):\n",
    "            model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "            model.reset_states()\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # Step 5\n",
    "    def forecast_lstm(model, batch_size, X):\n",
    "        X = X.reshape(1, 1, len(X))\n",
    "        yhat = model.predict(X, batch_size=batch_size)\n",
    "        return yhat[0,0]\n",
    "    \n",
    "    #Step 6\n",
    "    def invert_scale(scaler, X, value):\n",
    "        new_row = [x for x in X] + [value]\n",
    "        array = np.array(new_row)\n",
    "        array = array.reshape(1, len(array))\n",
    "        inverted = scaler.inverse_transform(array)\n",
    "        return inverted[0, -1]\n",
    "\n",
    "    # Now use all the above functions\n",
    "    supervised = timeseries_to_supervised(X,1)\n",
    "    supervised_values = supervised.values\n",
    "    train, test = supervised_values[0:-days_out], supervised_values[-days_out:]\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    train_reshaped = train_scaled[:,0].reshape(len(train_scaled),1,1)\n",
    "    lstm_model = fit_lstm(train_scaled,1,nb_epoch,neurons)\n",
    "    \n",
    "    #Step 7\n",
    "    predictions = []\n",
    "    for i in range(len(test_scaled)):\n",
    "        #Make one step forecast\n",
    "        X, y = test_scaled[i,0:-1], test_scaled[i,-1]\n",
    "        yhat = forecast_lstm(lstm_model,1,X)\n",
    "        #invert scaling\n",
    "        yhat = invert_scale(scaler, X, yhat)\n",
    "        #store forecast\n",
    "        predictions.append(yhat)\n",
    "    \n",
    "    # Step 8\n",
    "    # This part plugs it back into the signal\n",
    "    predictions_series = pd.Series(predictions, index = original_time_series.index[-days_out:])\n",
    "    a = original_time_series.loc[original_time_series.index[-(days_out+1):]]['Price']\n",
    "    b = np.exp(predictions_series)\n",
    "    full_predictions = pd.DataFrame(a*b,columns=['Predicted with Neural Nets']).dropna()\n",
    "    df = pd.concat([original_time_series.loc[original_time_series.index[-days_out:]],full_predictions],axis=1)\n",
    "    error = str(np.sqrt(mean_squared_error(df['Price'],df['Predicted with Neural Nets'])))\n",
    "    print(\"Neural Net Root Mean Squared Error: \",error)\n",
    "    df.index.name=None\n",
    "    df[['Price','Predicted with Neural Nets']] = df[['Price','Predicted with Neural Nets']].apply(lambda x: round(x,2))\n",
    "    return df\n",
    "\n",
    "print('Bitcoin Neural Net Predictions')\n",
    "bitcoin_NN = Neural_Net_predictions(bitcoin,bits_log_shift,days_out=7,nb_epoch=55,neurons=175)\n",
    "bitcoin_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources for information and tutorials that helped me write this analysis**:\n",
    "1. The Application of Time Series Modelling and Monte Carlo Simulation: Forecasting Volatile Inventory Requirements By Robert Davies, Tim Coole, David Osipyw, https://file.scirp.org/pdf/AM_2014050513382674.pdf\n",
    "\n",
    "2. How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course), by Jason Brownlee; https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/\n",
    "\n",
    "3. Statistical forecasting:\n",
    "notes on regression and time series analysis, By Robert Nau, Duke University https://people.duke.edu/~rnau/411arim.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
